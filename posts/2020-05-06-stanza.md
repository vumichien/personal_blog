---
layout: post
title: Overview | Stanza | New Python Natural Language Processing Library | Stanford
subtitle: This topic summarizes the new library Python Natural Language from scratch with applications
cover-img: /assets/img/NLP.jpg
share-img: /assets/img/NLP.jpg
tags: [Machine learning, NLP, Data science]
---
# Background ðŸ“™

Recently, The Stanford NLP Group released Stanza : A Python Natural Language Processing Toolkit for Many Human Languages [1] that introduced an open source Python natural language processing. Compared to existing toolkits for NLP task, researchers stated that:
> Stanza fully supported neural pipeline for text analysis, including tokenization, multiword token expansion, lemmatization, part-of-speech, morphological feature tagging, dependency parsing, and named entity recognition

The toolkit is built on top of the PyTorch library with support for using GPU and pre-trained neural models. Furthermore, Stanza assists **66 human languages** which was trained on a total of **112 datasets**, including the Universal Dependencies treebanks and other multilingual corpora. The result showed that the same neural architecture generalizes well and achieves competitive performance on all languages tested. You can run a demo here. Up to now, the popular NLP toolkits such as **CoreNLP** [2], **FLAIR** [3], **spaCy** [4], and **UDPipe** [5] which has been using in a wide variety of applications, they also suffer from several limitations: few number of supported major languages; under-optimized model and some tools lacking the ability to process raw text from diverse sources. Compared to these NLP toolkits, Stanza has the following advantages:

* **From raw text to annotations**: Stanza features a fully neural pipeline which takes raw text as input, and produces annotations to any specific purpose.
* **Multilingual**: Stanza architectural design is language-agnostic and data-driven, which allows us to release models supporting 60+ languages, by training the pipeline on the Universal Dependencies (UD) treebanks and other multilingual corpora. Comparing the system against UDPipe and spaCy on treebanks of five major languages where the corresponding pretrained models are publicly available, the result showed better performance over others. Therefore, you can strongly believe and confident to use the result for your downstream application. You can find more information in their [paper](https://arxiv.org/abs/2003.07082)
* **State-of-the-art performance**: Stanza neural pipeline adapts well to text of different genres, achieving state-of-the-art or competitive performance at each step of the pipeline.

# Getting Started ðŸ› ï¸
First of all you need to install Stanza . Stanza supports Python 3.6 or later. With pip, you can easily run this code:
```
pip install stanza
```
Or with Anaconda:
```
conda install -c stanfordnlp stanza
```

# Building a Pipeline âš™ï¸

Stanza provides simple, flexible, and unified interfaces for downloading and running various NLP models, you can refer to the Downloading Models and Pipeline pages. At a high level, to start annotating text, firstly, you need to initialize a Pipeline, which pre-loads and chains up a series of Processors, with each processor performing a specific NLP task (e.g., tokenization, dependency parsing, or named entity recognition). Moreover, many options such as control devices (CPU or GPU), logging from the Pipeline or specify model path, etc. can be customized.

## Download model language
Literally saying, it is essential in most of the cases to download the pre-trained model language from Stanza before conducting further training with NLP tasks. Itâ€™s just simple with the stanza.download command. The language can be specified with either a full language name (e.g., "Japanese"), or a short code (e.g., "ja"). To get more information of supported language model you can take a look in [here](https://stanfordnlp.github.io/stanza/models.html#human-languages-supported-by-stanza)
```
# download pretrain model of Japanese language
print("Downloading Japanese model...")
stanza.download('ja')
# Similarly, with another language model (ex. French model)
# Note that you can use verbose=False to turn off all printed messages
print("Downloading French model...")
stanza.download('fr', verbose=False)
```
## TokenizeProcessor
Then, Tokenizing text into the meaning number for computer is the next step. Definitely, Stanza supports a lot of processors in NLP pipeline but in some case, you only need the word tokenize and performs sentence segmentation, `TokenizeProcessor` will help you to do these stuffs, so that downstream annotation can happen at the sentence level. Here is an example for download and pipeline interface
```
import stanza
stanza.download('ja')

nlp = stanza.Pipeline('ja',processors='tokenize')
doc = nlp("ã€Œç ºæ³¢ãƒãƒ¥ãƒ¼ãƒªãƒƒãƒ—å…¬åœ’ã€ã¯ã€ãƒãƒ¥ãƒ¼ãƒªãƒƒãƒ—ã§æœ‰åå…¬åœ’ã§ã™ã€‚æ¯Žå¹´ï¼”æœˆã®çµ‚ã‚ã‚Šã‹ã‚‰ï¼•æœˆã®åˆã‚ã®ä¼‘ã¿ã«ã‚¤ãƒ™ãƒ³ãƒˆã‚’è¡Œã£ã¦ã„ã¾ã™")
for i, sentence in enumerate(doc.sentences):
  print(f'====== Sentence {i+1} tokens =======')
  print(*[f'id: {token.id}\ttext: {token.text}' for token in sentence.tokens], sep='\n')
```
## POSProcessor
Stanza also supplies a processor to label the token with their universal POS (UPOS) tags, treebank-specific POS (XPOS) tags, and universal morphological features (UFeats). The part-of-speech tags can be accessed via the upos(pos) and xpos fields of each Word from the Sentences. Note: `POSProcessor` requires the `TokenizeProcessor` and `MWTProcessor` in the pipeline. `MWTProcessor` is only applicable to some languages. You can check here to know more information.
```
nlp = stanza.Pipeline(lang='ja', processors='tokenize,mwt,pos', verbose=False)
doc = nlp("ã€Œç ºæ³¢ãƒãƒ¥ãƒ¼ãƒªãƒƒãƒ—å…¬åœ’ã€ã¯ã€ãƒãƒ¥ãƒ¼ãƒªãƒƒãƒ—ã§æœ‰åå…¬åœ’ã§ã™ã€‚æ¯Žå¹´ï¼”æœˆã®çµ‚ã‚ã‚Šã‹ã‚‰ï¼•æœˆã®åˆã‚ã®ä¼‘ã¿ã«ã‚¤ãƒ™ãƒ³ãƒˆã‚’è¡Œã£ã¦ã„ã¾ã™")
for i, sent in enumerate(doc.sentences):
  print(f'====== Sentence {i+1} tokens =======')
  print(*[f'word: {word.text}\tupos: {word.upos}\txpos: {word.xpos}' for word in sent.words], sep='\n')
```
## LemmaProcessor
As other NLP toolkits, Stanza also supports Lemmatisation process, it called `LemmaProcessor` .`TokenizeProcessor`, `MWTProcessor`, and `POSProcessor` are the requisite in the pipeline to run `LemmaProcessor`. Lemmatizing words in a sentence and accessing their lemmas afterwards can be done as below.
```
nlp = stanza.Pipeline(lang='ja', processors='tokenize,mwt,pos,lemma', verbose=False)
doc = nlp("ã€Œç ºæ³¢ãƒãƒ¥ãƒ¼ãƒªãƒƒãƒ—å…¬åœ’ã€ã¯ã€ãƒãƒ¥ãƒ¼ãƒªãƒƒãƒ—ã§æœ‰åå…¬åœ’ã§ã™ã€‚æ¯Žå¹´ï¼”æœˆã®çµ‚ã‚ã‚Šã‹ã‚‰ï¼•æœˆã®åˆã‚ã®ä¼‘ã¿ã«ã‚¤ãƒ™ãƒ³ãƒˆã‚’è¡Œã£ã¦ã„ã¾ã™")
for i, sent in enumerate(doc.sentences):
  print(f'====== Sentence {i+1} tokens =======')
  print(*[f'word: {word.text+" "}\tlemma: {word.lemma}' for word in sent.words], sep='\n')
```
## DepparseProcessor
To check how well you model can understand each word in your full sentence, you can use `DepparseProcessor` which provides an accurate syntactic dependency parser.
Remember, `DepparseProcessor` requires `TokenizeProcessor`, `MWTProcessor`, `POSProcessor` and `LemmaProcessor` in the pipeline. The head index of each Word can be accessed by the property head, and the dependency relation between the words deprel . This is example:
```
nlp = stanza.Pipeline(lang='ja', processors='tokenize,mwt,pos,lemma,depparse', verbose = False)
doc = nlp("ã€Œç ºæ³¢ãƒãƒ¥ãƒ¼ãƒªãƒƒãƒ—å…¬åœ’ã€ã¯ã€ãƒãƒ¥ãƒ¼ãƒªãƒƒãƒ—ã§æœ‰åå…¬åœ’ã§ã™ã€‚æ¯Žå¹´ï¼”æœˆã®çµ‚ã‚ã‚Šã‹ã‚‰ï¼•æœˆã®åˆã‚ã®ä¼‘ã¿ã«ã‚¤ãƒ™ãƒ³ãƒˆã‚’è¡Œã£ã¦ã„ã¾ã™")
for i, sent in enumerate(doc.sentences):
  print(f'====== Sentence {i+1} tokens =======')
  print(*[f'id: {word.id}\tword: {word.text}\thead id: {word.head}\thead: {sent.words[word.head-1].text if word.head > 0 else "root"}\tdeprel: {word.deprel}'  for word in sent.words], sep='\n')
```
## NERProcessor
The last processor I want to introduce is `NERProcessor` . Recognize named entities process for all token spans in the corpus, NER, is a familiar phrase in NLP. To apply `NERProcesso`r in Stanza, we need `TokenizeProcessor` in the pipeline. Unfortunately, until now, Stanza only support for 8 languages. You can check out here to found all supported languages along with their training datasets. So that, we can find NER for Japanese language. However we can find another solution for this problem. But at first, I will show you the simple code to run NERProcessor if your language is supported.
```
import stanza
stanza.download('fr')
nlp = stanza.Pipeline(lang='fr', processors='tokenize,ner', verbose = False)
doc = nlp("JÃ©sus-Christ offert en sacrifice sur la croix et prÃ©sent par l'eucharistie dans le sacrifice de la messe.")
print(*[f'id: {token.id} \ttoken: {token.text}\tner: {token.ner}' for sent in doc.sentences for token in sent.tokens], sep='\n')
```
# Final thoughts ðŸ“‹
So far, we can see that Stanza are still in development and there are many rooms to improve for the future. At the moment, you can use the pre-trained model of Stanza for any tasks of NLP like machine translation, sentiment analysis or question answering, etc. However, Stanza was designed to optimize for accuracy of its predictions, and consequently leads to a small trade off in cost of computational efficiency. You can run and check out with your data.
Happy learning!

# References
[1] Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton and Christopher D. Manning. 2020. Stanza: A Python Natural Language Processing Toolkit for Many Human Languages. In Association for Computational Linguistics (ACL) System Demonstrations. 2020.<br/>
[2] Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Association for Computational Linguistics (ACL) System Demonstrations. <br/>
[3] Alan Akbik, Tanja Bergmann, Duncan Blythe, Kashif Rasul, Stefan Schweter, and Roland Vollgraf. 2019. FLAIR: An easy-to-use framework for state-of-theart NLP. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations). Association for Computational Linguistics.<br/>
[4] https://spacy.io<br/>
[5] Milan Straka. 2018. UDPipe 2.0 prototype at CoNLL 2018 UD shared task. In Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Association for Computational Linguistics.<br/>
