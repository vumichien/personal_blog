---
layout: post
title: Push the accuracy of Machine learning model with Numerical Variable Transformation in Pytorch
subtitle: This story provides complete guide to implement Transformation Technique and improve accuracy with code in Pytorch
cover-img: /assets/img/accuracy.jpg
share-img: /assets/img/accuracy.jpg
tags: [Machine learning, Data science, Feature Engineering, Pytorch]
---
# Overview üìô

Many machine learning practitioners get stuck in improving their model performance on prediction. 
They mainly focus in using more powerful SOTA model and trade off with longer training time. 
However, time is precious in Machine learning and the model‚Äôs performance can achieve better result on much faster speed through solely a simple step, Data Reprocessing. 
This step is often forgot or underestimated.

Some of examples of Data Preprocessing include filling out Missing Value with Imputation Technique [1], converting or encoding Categorical Variable using Encoding [2] and transforming to Numerical Variable by Standardizing or Normalizing [3]. 
Thus, this article want to focus on giving the powerful methodology for Transforming numerical variable to help machine learning model capture the meaningful information inside the data.

# Transformation Technique üìÑ

Indeed, many machine learning algorithms (like linear regression and logistic regression) perform better when the distribution of variables is normal (or Gaussian distribution, named after Carl Friedrich Gauss), in other words, the performance degrades for variables that have non-standard probability distributions.

Normal distribution in the bell shape is the probability function of a continuous random variable that appears in various instances in real life: the heights of people, the blood pressure or the scores on a test. Other nonlinear algorithms may not have this fact, yet often perform better when numerical variables have a Gaussian distribution.

Various useful techniques presented in this article can be used to transform variable into Gaussian distribution, or more likely to be Gaussian. Those methods are Quantile Transforms, Power Transforms and Discretization Transforms.

* **Quantile Transforms** [4] is a nonlinear transformation that replaces the original value of a feature with the mean of all other features‚Äô values at the same rank or quantile. That sounds fancy but is really super simple. As below illustration, firstly, the data is sorted by their values. Then you compute the average across all columns of the sorted data set, and replace the original values with the average one. Finally, return the dataset to the original order but keep the normalized values.
![quantile_fundametal](/assets/img/quantile_fundametal.png)
* **Power Transform** [5] belongs to the family of parametric, monotonic transformations that used to stabilize variance and minimize the skewness issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Heteroskedaticity, on the other hand, happens when variance around the regression line is not the same for all values of the predictor variable. There are two popular approaches for such automatic power transforms: Box-Cox Transform and Yeo-Johnson Transform. Lambda is known as a hyper-parameter that use to control the nature of the Power transform, as well as, is the best approach to transform a variable to a Gaussian probability distribution. Below are some common values for lambda:
![power_lambda](/assets/img/power_lambda.png)
* **Discretization Transforms** [6] process converts quantitative data into qualitative data, that means, from numerical attributes to discrete or nominal attributes with a finite number of intervals, obtaining a non-overlapping partition of a continuous domain. For both users and experts, discrete features are easier to understand, use, and explain. In practice, discretization can be viewed as a data reduction method since it maps data from a huge spectrum of
numeric values to a greatly reduced subset of discrete values, that makes learning more accurate and faster. Discretization techniques include many methodologies like binning, histogram, cluster, decision tree, correlation, etc.. Note: On the linearly non-separable datasets, feature discretization increases the performance of linear classifiers model. However, on the linearly separable dataset, feature discretization will decreases the performance of linear classifiers model.

# Experiment with Boston Housing Data set

In this tutorial, I will use the Boston Housing data set which contains information of various houses in Boston. There are 506 samples and 13 feature variables. The objective is to predict the house‚Äôs prices using the given features. First is importing and loading data

{% highlight python linenos %}
import pandas as pd 
DATASET_URL = "https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv"
# read data
DATA_FILENAME = "Boston_Housing.csv"
TARGET_COLUMN = 'medv' 
dataframe = pd.read_csv(DATASET_URL)
dataframe.drop('medv', axis=1).hist();
{% endhighlight %}

![accuracy_base](/assets/img/acuuracy_base.png)

# Baseline model

Next I will create a simple linear model for predict the prices of the house.

{% highlight python linenos %}
import torch.nn as nn
import torch.nn.functional as F

class HousingModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(input_size, output_size)
        
    def forward(self, xb):
        out = self.linear(xb)
        return out
    
    def training_step(self, batch):
        inputs, targets = batch 
        out = self(inputs)                 # Generate predictions
        loss = F.mse_loss(out, targets)    # Calculate loss
        return loss
    
    def validation_step(self, batch):
        inputs, targets = batch 
        out = self(inputs)                 # Generate predictions
        loss = F.mse_loss(out, targets)    # Calculate loss
        return loss.detach()
    
    def epoch_end(self, epoch, epochs, result):
        print("Epoch {}/{} \n Train_loss: {:.4f}  Validation_loss: {:.4f}".format(epoch+1, epochs, result['train_loss'], result['val_loss'] ))

{% endhighlight %}

Then, I load input and output data to PyTorch dataset and train the model.

{% highlight python linenos %}
# Hyperparameters
batch_size=64
learning_rate=5e-7
input_size=13
output_size=1

# Convert from Pandas dataframe to numpy arrays
inputs = dataframe.drop('medv', axis=1).values
targets = dataframe[['medv']].values

# Convert to PyTorch dataset
dataset = TensorDataset(torch.tensor(inputs, dtype=torch.float32), torch.tensor(targets, dtype=torch.float32))
train_ds, val_ds = random_split(dataset, [406, 100])

train_loader = DataLoader(train_ds, batch_size, shuffle=True)
val_loader = DataLoader(val_ds, batch_size*2)

# Evaluation function
def evaluate(model, train_loader, val_loader):
    train_loss_batch = [model.validation_step(batch) for batch in train_loader]
    train_loss = torch.stack(train_loss_batch).mean()
    val_loss_batch = [model.validation_step(batch) for batch in val_loader]
    val_loss = torch.stack(val_loss_batch).mean()
    return {'train_loss': train_loss.item(), 'val_loss': val_loss.item()}
  
# Training function
def train(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):
    history = []
    optimizer = opt_func(model.parameters(), lr)
    print('Training...')
    for epoch in range(epochs):
        # Training Phase 
        for batch in train_loader:        
            loss = model.training_step(batch)
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
        # Validation phase
        result = evaluate(model, train_loader, val_loader)
        model.epoch_end(epoch,epochs, result)
        history.append(result)
    return history
 
model = HousingModel()
history = train(10, learning_rate, model, train_loader, val_loader)
{% endhighlight %}

# Quantile Transforms
Next, I will use Quantile Transforms to make the numerical variables more-Gaussian. I can apply the Quantile transform using the QuantileTransformer class in sklearn and set the ‚Äú output_distribution‚Äù argument to ‚Äú normal‚Äù. I must also set the ‚Äú n_quantiles ‚Äú argument to a value less than the number of observations in the training dataset, in this case, 100.

{% highlight python linenos %}
from sklearn.preprocessing import QuantileTransformer 
transformer = QuantileTransformer(n_quantiles=100, output_distribution='normal')
inputs = transformer.fit_transform(inputs_raw)
{% endhighlight %}

![accuracy_quantile](/assets/img/acuuracy_quantile.png)

After transforming an input variable to have a normal probability distribution by Quantile Transforms, the input distribution look like this figure.
Then, I use the same linear model above to predict the prices of the house in Boston. 
Note: After transforming the input data with Transformation Technique, I can set higher learning rate (in this case learning rate = 1e-2) to fasten the convergence to global minimum. For the raw data, if the learning rate is set too high, we will face Non-Convergence Error.

# Power Transforms
In this demonstration, I used Box-Cox Transform technique of Power Transforms.
Note: Box-Cox Transform assumes the values of the input variable to which it is applied are strictly positive. That means 0 and negative values cannot be used (in this case I can use Yeo-Johnson Transform). In the input dataset, I have negative data, therefore, I have to use MixMaxScaler transformbefore feed into Box-Cox Transform.

{% highlight python linenos %}
from sklearn.preprocessing import PowerTransformer, MinMaxScaler 
transformer = PowerTransformer(method='box-cox')
pipeline = Pipeline(steps=[('s', scaler),('t', transformer)])
scaler = MinMaxScaler(feature_range=(1, 2)) 
inputs = pipeline.fit_transform(inputs_raw)
{% endhighlight %}

![accuracy_power](/assets/img/acuuracy_power.png)

# Discretization Transform
Last but not least, I will apply the powerful Transformation Technique, Discretization Transform. A uniform Discretization Transform will preserve the probability distribution of each input variable but will make it discrete with the specified number of ordinal groups or labels. I can apply the uniform discretization transform using the KBinsDiscretizer class and set the ‚Äú strategy‚Äù argument to ‚Äú uniform.‚Äù I also set the number of bins set via the ‚Äú n_bins ‚Äú argument to 10.

{% highlight python linenos %}
from sklearn.preprocessing import KBinsDiscretizer 
transformer = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')
inputs = transformer.fit_transform(inputs_raw)
{% endhighlight %}

![accuracy_discretization](/assets/img/acuuracy_discretization.png)

# Final thought üìã
This article wants to demonstrate some techniques of transforming the numerical input data to like-Gaussian distribution. However, this technique will not improve the model‚Äôs performance if the input data has already been similar to Normal Distribution. This experiment only use the default hyper parameter, you can freely to change and apply to your data to get the optimum results. For further reading, you can refer to Reference part below. Lastly, I want to summarize the result of all experiments in this article.
Peace!<br/>
You can check more detail in [here](https://towardsdatascience.com/push-the-accuracy-of-machine-learning-model-with-numerical-variable-transformation-in-pytorch-9f56c56203fd)
![accuracy_sumary](/assets/img/accuracy_sumary.png)

# References
[1] Stef vanBuuren, Flexible Imputation of Missing Data, Second Edition, CRC Press, 2018. <br/>
[2] Suresh Kumar Mukhiya Usman Ahmed, Hands-On Exploratory Data Analysis with Python: Perform EDA techniques to understand, summarize, and investigate your data, Packt Publishing Ltd, 2020. <br/>
[3] C.Y. Joanne Peng, Data Analysis Using SAS, Chapter 7, Data Transformation, 2009. <br/>
[4] Amaratunga, D., Cabrera, J. Analysis of Data From Viral DNA Microchips. J Amer Statist Assoc 96, 2001.<br/>
[5] S. Gluzman, V.I. Yukalov, Self-similar power transforms in extrapolation problems, Statistical Mechanics, 2006.<br/>
[6] Huan Liu, Farhad Hussain, Chew Lim Tan & Manoranjan Dash, Discretization: An Enabling Technique, 2002.<br/>
[7] Salvador Garc√≠a ; Juli√°n Luengo ; Jos√© Antonio S√°ez ; Victoria L√≥pez ; Francisco Herrera, A Survey of Discretization Techniques: Taxonomy and Empirical Analysis in Supervised Learning, 2013.<br/>
